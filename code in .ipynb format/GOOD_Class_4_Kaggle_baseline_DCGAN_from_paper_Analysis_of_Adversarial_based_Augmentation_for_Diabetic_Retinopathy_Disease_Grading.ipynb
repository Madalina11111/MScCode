{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GOOD Class 4 Kaggle baseline DCGAN from paper Analysis of Adversarial based Augmentation for Diabetic Retinopathy Disease Grading.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5rYygJLOx21"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot\n",
        "import os\n",
        "import keras\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from skimage.io import imread\n",
        "from PIL import Image #PIL is no longer maintained and it is not compatible with Python 3.X. To use Pillow instead\n",
        "from numpy import load\n",
        "from os import listdir\n",
        "from numpy import savez_compressed\n",
        "from numpy import asarray\n",
        "from google.colab import files\n",
        "from keras import Sequential\n",
        "from keras.layers import *\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adam, SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2WeQZtHPH52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bebb9241-ebd4-45c2-c412-588101679d7d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR4I2hPLPKxm"
      },
      "source": [
        "#load images from npz\n",
        "def load_real_samples(filename):\n",
        "  data = load(filename)\n",
        "  X = data['arr_0']\n",
        "  X = X.astype('float32')\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9k8F2gsmFEB"
      },
      "source": [
        "def discriminator(img_shape):\n",
        "  depth = 64\n",
        "  dropout = 0.3\n",
        "  lrL2 = 1e-7\n",
        "\n",
        "  D = Sequential()\n",
        "  D.add(keras.Input(shape=(128,128,3)))\n",
        "\n",
        "  D.add(Conv2D(depth*1, 5, strides=2, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "  D.add(BatchNormalization())\n",
        "  D.add(ReLU())\n",
        "  D.add(Dropout(dropout))\n",
        " \n",
        "  D.add(Conv2D(depth*2, 5, strides=2, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "  D.add(BatchNormalization())\n",
        "  D.add(ReLU())\n",
        "  D.add(Dropout(dropout))\n",
        "\n",
        "  D.add(Conv2D(depth*4, 5, strides=2, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "  D.add(BatchNormalization())\n",
        "  D.add(ReLU())\n",
        "  D.add(Dropout(dropout))\n",
        "\n",
        "  D.add(Conv2D(depth*8, 5, strides=1, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "  D.add(BatchNormalization())\n",
        "  D.add(ReLU())\n",
        "  D.add(Dropout(dropout))\n",
        "\n",
        "  D.add(Flatten())\n",
        "  D.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  return D\n",
        "\n",
        "my_disc = discriminator((128, 128, 3))\n",
        "my_disc.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adp7PHYnhaGP"
      },
      "source": [
        "def generator():\n",
        "\n",
        "    G = Sequential()\n",
        "    depth = 64*4\n",
        "    img_rows = 128\n",
        "    dim = int(img_rows / 8 )\n",
        "    dropout = 0.3\n",
        "    lrL2 = 1e-7\n",
        "\n",
        "    G.add(keras.Input(shape=(100,1)))\n",
        "    G.add(Dense(512))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "    G.add(Reshape((dim, dim, 200)))\n",
        "\n",
        "    G.add(UpSampling2D(size=(2, 2)))\n",
        "\n",
        "    G.add(Conv2D(int(depth), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "    G.add(Conv2D(int(depth/2), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "\n",
        "    G.add(UpSampling2D(size=(2, 2)))\n",
        "\n",
        "    G.add(Conv2D(int(depth/4), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "    G.add(Conv2D(int(depth/8), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "\n",
        "    G.add(UpSampling2D(size=(2, 2)))\n",
        "\n",
        "    G.add(Conv2D(int(depth/16), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "    G.add(Conv2D(int(depth/32), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "\n",
        "    G.add(Conv2D(3, 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(ReLU())\n",
        "    return G\n",
        "\n",
        "gen_model = generator()\n",
        "gen_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O3fwigZZEu7"
      },
      "source": [
        "#generate synthetic data using the generator\n",
        "def generate_fake_samples_using_generator(g_model, noise_dim, n_samples):\n",
        "  flattened_noise_dim = noise_dim[0]\n",
        "  x = np.random.randn(n_samples * flattened_noise_dim)\n",
        "  x_data = x.reshape(n_samples, noise_dim[0], noise_dim[1])\n",
        "  x = g_model.predict(x_data)\n",
        "  y = np.zeros((n_samples, 1))\n",
        "\n",
        "  return x, y\n",
        "\n",
        "#create GAN architecture\n",
        "def gan(g_model , d_model):\n",
        "  d_model.trainable = False\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(g_model)\n",
        "  model.add(d_model)\n",
        "  lrAdv = 1e-4\n",
        "  decayAdv = 1e-6\n",
        "  optimizer_adver = Adam(lrAdv)#, decay = decayAdv)\n",
        "\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer = optimizer_adver)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuTPNAe5ZEes"
      },
      "source": [
        "def get_real_samples (dataset, n_samples):\n",
        "  indicies = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "  X = dataset[indicies]\n",
        "  y = np.ones((n_samples, 1))\n",
        "  return X, y\n",
        "\n",
        "def generate_random_vectors(noise_dim, batch_size):\n",
        "  x = np.random.randn(batch_size * noise_dim[0] * noise_dim[1])\n",
        "  x_data = x.reshape(batch_size, noise_dim[0], noise_dim[1])\n",
        "  return x_data\n",
        "\n",
        "def plot_history(d_real_loss, d_fake_loss, g_loss):\n",
        "  pyplot.subplot(1, 1, 1)\n",
        "  pyplot.plot(d_real_loss, label='d-real')\n",
        "  pyplot.plot(d_fake_loss, label='d-fake')\n",
        "  pyplot.plot(g_loss, label='gen')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "  pyplot.close()\n",
        "\n",
        "def plot_history_accuracy(d_real_acc, d_fake_acc):\n",
        "  pyplot.subplot(1, 1, 1)\n",
        "  pyplot.plot(d_real_acc, label='acc-real')\n",
        "  pyplot.plot(d_fake_acc, label='acc-fake')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "  pyplot.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiDAN3I6PYaN"
      },
      "source": [
        "#train the gan\n",
        "import cv2\n",
        "OUTPUT_DIR = \"/content/gdrive/My Drive/img_retina_Kaggle_class_4_state_of_art_GAN\"\n",
        "\n",
        "#train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, noise_dim, epochs, batch_size):\n",
        "  drealLoss = []\n",
        "  dfakeLoss = []\n",
        "  gLoss = []\n",
        "  accuracies_real = []\n",
        "  accuracies_fake = []\n",
        "  inx = []\n",
        "  counter = 0\n",
        "  batches = int(dataset.shape[0] / batch_size)\n",
        "\n",
        "  #  for each epoch\n",
        "  for i in range(epochs):\n",
        "    print(\"Epoch: \" + str(i))\n",
        "    for j in range(batches):\n",
        "      X_real, y_real = get_real_samples(dataset, batch_size)\n",
        "      X_fake, y_fake = generate_fake_samples_using_generator(g_model, noise_dim, batch_size)\n",
        "      d_loss_real, d_accuracy_real = d_model.train_on_batch(X_real, y_real)\n",
        "      d_loss_fake, d_accuracy_fake = d_model.train_on_batch(X_fake, y_fake)\n",
        "      X_gan = generate_random_vectors(noise_dim, batch_size)\n",
        "      y_gan = np.ones((batch_size, 1))\n",
        "      gan_loss = gan_model.train_on_batch(X_gan , y_gan)\n",
        "      print(\"Batch: \" + str(j) + \" generator loss: \" + str(gan_loss) + \", discriminator real loss: \" + str(d_loss_real)  + \", discriminator fake loss: \" + str(d_loss_fake))\n",
        "      drealLoss.append(d_loss_real)\n",
        "      dfakeLoss.append(d_loss_fake)\n",
        "      gLoss.append(gan_loss)\n",
        "      accuracies_real.append(d_accuracy_real)\n",
        "      accuracies_fake.append(d_accuracy_fake)\n",
        "        \n",
        "    if ((i+1) % 10) == 0:\n",
        "\n",
        "      if(os.path.exists(OUTPUT_DIR + \"/\" + str(i)) == False):\n",
        "        os.mkdir(OUTPUT_DIR + \"/\" + str(i))\n",
        "      noise_dim = (100, 1)\n",
        "      n_samples = 165\n",
        "      x,y = generate_fake_samples_using_generator(g_model, noise_dim, n_samples)\n",
        "      for m in range(n_samples):\n",
        "        this_image = x[m]\n",
        "        this_image = this_image.astype(np.uint8)   \n",
        "        image_name = str(i) + \"_\" + str(j)+\"_\"+str(m)\n",
        "        im2 = Image.fromarray(this_image)\n",
        "        im2.save(\"{}/{}.jpeg\".format(OUTPUT_DIR + \"/\" + str(i),image_name))\n",
        "\n",
        "    plot_history(drealLoss, dfakeLoss, gLoss)\n",
        "    plot_history_accuracy(accuracies_real, accuracies_fake)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4mdBnQr0Zjm"
      },
      "source": [
        "noise_dim = (100, 1)\n",
        "shape=(128,128,3)\n",
        "\n",
        "disc_model =  discriminator(shape)\n",
        "lrDisc = 1e-4\n",
        "decayDisc = 1e-5 #2e-5\n",
        "\n",
        "\n",
        "optimizer_disc = SGD(lrDisc)#, decay=decayDisc) #, momentum = 0.9) #, decay=decayDisc)\n",
        "\n",
        "disc_model.compile(loss = 'binary_crossentropy', optimizer=optimizer_disc, metrics=['accuracy'])\n",
        "\n",
        "generator_model = generator()\n",
        "gan_model = gan(generator_model, disc_model)\n",
        "\n",
        "dataset = load_real_samples(\"/content/gdrive/My Drive/MSc. project/npz files conference/Kaggle_training_images_in_order_preprocessed_class4_normalized3.npz\")\n",
        "\n",
        "epochs=500\n",
        "batch_size= 4 \n",
        "\n",
        "train(generator_model, disc_model, gan_model, dataset, noise_dim, epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}