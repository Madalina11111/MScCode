{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GOOD Class 4 Kaggle baseline DCGAN from paper Analysis of Adversarial based Augmentation for Diabetic Retinopathy Disease Grading.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5rYygJLOx21"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot\n",
        "import os\n",
        "import keras\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from skimage.io import imread\n",
        "from PIL import Image #PIL is no longer maintained and it is not compatible with Python 3.X. To use Pillow instead\n",
        "from numpy import load\n",
        "from os import listdir\n",
        "from numpy import savez_compressed\n",
        "from numpy import asarray\n",
        "from google.colab import files\n",
        "from keras import Sequential\n",
        "from keras.layers import *\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adam, SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP3emJ7ZwZNh"
      },
      "source": [
        "#to do:\n",
        "# *from the training folder for kaggle, select the kaggle images with every class. Put them in a folder by class\n",
        "# *make a npz file with the images for every class\n",
        "#run the baseline GAN for every class\n",
        "#store fake images of every class in a separate folder\n",
        "#decide how many fake images from every class I want to use for the sclassification. Probably 20% of the real images of every class, in order to maintain the proportion of every class\n",
        "#Create a folder with real + fake images for all the classes\n",
        "#create a npz file with the images from all the folders. Make sure that the images are shuffled. For ex, real images should be mixed with synthetic images and images from different classes should be shuffled.\n",
        "#create a copy of the classifier file to use the new npz file\n",
        "#do classification with the classifier that I already have "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2WeQZtHPH52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bebb9241-ebd4-45c2-c412-588101679d7d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR4I2hPLPKxm"
      },
      "source": [
        "#load images from npz\n",
        "def load_real_samples(filename):\n",
        "  # load dataset\n",
        "  data = load(filename)\n",
        "  # extract numpy array\n",
        "  X = data['arr_0']\n",
        "  # convert from ints to floats\n",
        "  X = X.astype('float32')\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9k8F2gsmFEB"
      },
      "source": [
        "def discriminator(img_shape):\n",
        "  depth = 64\n",
        "  dropout = 0.3\n",
        "  lrL2 = 1e-7\n",
        "\n",
        "  D = Sequential()\n",
        "  D.add(keras.Input(shape=(128,128,3)))\n",
        "\n",
        "  D.add(Conv2D(depth*1, 5, strides=2, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "  D.add(BatchNormalization())\n",
        "  D.add(ReLU())\n",
        "  D.add(Dropout(dropout))\n",
        " \n",
        "  D.add(Conv2D(depth*2, 5, strides=2, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "  D.add(BatchNormalization())\n",
        "  D.add(ReLU())\n",
        "  D.add(Dropout(dropout))\n",
        "\n",
        "  D.add(Conv2D(depth*4, 5, strides=2, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "  D.add(BatchNormalization())\n",
        "  D.add(ReLU())\n",
        "  D.add(Dropout(dropout))\n",
        "\n",
        "  D.add(Conv2D(depth*8, 5, strides=1, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "  D.add(BatchNormalization())\n",
        "  D.add(ReLU())\n",
        "  D.add(Dropout(dropout))\n",
        "\n",
        "  D.add(Flatten())\n",
        "  D.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  return D\n",
        "\n",
        "my_disc = discriminator((128, 128, 3))\n",
        "my_disc.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adp7PHYnhaGP"
      },
      "source": [
        "def generator():\n",
        "\n",
        "    G = Sequential()\n",
        "    depth = 64*4\n",
        "    img_rows = 128\n",
        "    dim = int(img_rows / 8 )\n",
        "    dropout = 0.3\n",
        "    lrL2 = 1e-7\n",
        "\n",
        "    G.add(keras.Input(shape=(100,1)))\n",
        "    G.add(Dense(512))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "    G.add(Reshape((dim, dim, 200)))\n",
        "\n",
        "    G.add(UpSampling2D(size=(2, 2)))\n",
        "\n",
        "    G.add(Conv2D(int(depth), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "    G.add(Conv2D(int(depth/2), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "\n",
        "    G.add(UpSampling2D(size=(2, 2)))\n",
        "\n",
        "    G.add(Conv2D(int(depth/4), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "    G.add(Conv2D(int(depth/8), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "\n",
        "    G.add(UpSampling2D(size=(2, 2)))\n",
        "\n",
        "    G.add(Conv2D(int(depth/16), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "    G.add(Conv2D(int(depth/32), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(ReLU())\n",
        "    G.add(Dropout(dropout))\n",
        "\n",
        "    G.add(Conv2D(3, 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))\n",
        "    G.add(ReLU())\n",
        "    return G\n",
        "\n",
        "gen_model = generator()\n",
        "gen_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O3fwigZZEu7"
      },
      "source": [
        "#generate synthetic data using the generator\n",
        "def generate_fake_samples_using_generator(g_model, noise_dim, n_samples):\n",
        "  \n",
        "\t# generate random data points of size noise_dim\n",
        "  flattened_noise_dim = noise_dim[0]\n",
        "  x = np.random.randn(n_samples * flattened_noise_dim)\n",
        "  # reshape into a batch of inputs for the network\n",
        "  x_data = x.reshape(n_samples, noise_dim[0], noise_dim[1])\n",
        "\t\n",
        "  # push data through the generator model\n",
        "  x = g_model.predict(x_data)\n",
        "\n",
        "  # set class labels to 0 for artificial data\n",
        "  y = np.zeros((n_samples, 1))\n",
        "\n",
        "  return x, y\n",
        "\n",
        "#create GAN architecture\n",
        "def gan(g_model , d_model):\n",
        "  # disable weight update on GAN model\n",
        "  d_model.trainable = False\n",
        "\n",
        "  # connect the generator with the discriminator\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(g_model)\n",
        "  model.add(d_model)\n",
        "  # compile model\n",
        "  lrAdv = 1e-4\n",
        "  decayAdv = 1e-6\n",
        "  optimizer_adver = Adam(lrAdv)#, decay = decayAdv)\n",
        "\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer = optimizer_adver)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuTPNAe5ZEes"
      },
      "source": [
        "def get_real_samples (dataset, n_samples):\n",
        "  #randomly generate indices to extract from training set\n",
        "  indicies = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "  #extract batch of slected images\n",
        "  X = dataset[indicies]\n",
        "  #set class label to 1\n",
        "  y = np.ones((n_samples, 1))\n",
        "  return X, y\n",
        "\n",
        "def generate_random_vectors(noise_dim, batch_size):\n",
        "  #generate random numbers from a normal distribution\n",
        "  x = np.random.randn(batch_size * noise_dim[0] * noise_dim[1])\n",
        "\n",
        "  #reshape into a batch of randomized vectors\n",
        "  #(each vector will be inputted to the generator and will output an image)\n",
        "  x_data = x.reshape(batch_size, noise_dim[0], noise_dim[1])\n",
        "  \n",
        "  return x_data\n",
        "\n",
        "# create a line plot of loss for the gan and save to file\n",
        "def plot_history(d_real_loss, d_fake_loss, g_loss):\n",
        "  # plot loss\n",
        "  pyplot.subplot(1, 1, 1)\n",
        "  pyplot.plot(d_real_loss, label='d-real')\n",
        "  pyplot.plot(d_fake_loss, label='d-fake')\n",
        "  pyplot.plot(g_loss, label='gen')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "  pyplot.close()\n",
        "\n",
        "def plot_history_accuracy(d_real_acc, d_fake_acc):\n",
        "  pyplot.subplot(1, 1, 1)\n",
        "  pyplot.plot(d_real_acc, label='acc-real')\n",
        "  pyplot.plot(d_fake_acc, label='acc-fake')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "  pyplot.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiDAN3I6PYaN"
      },
      "source": [
        "#train the gan\n",
        "import cv2\n",
        "OUTPUT_DIR = \"/content/gdrive/My Drive/img_retina_Kaggle_class_4_state_of_art_GAN\"\n",
        "\n",
        "#train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, noise_dim, epochs, batch_size):\n",
        "  drealLoss = []\n",
        "  dfakeLoss = []\n",
        "  gLoss = []\n",
        "  accuracies_real = []\n",
        "  accuracies_fake = []\n",
        "  inx = []\n",
        "  counter = 0\n",
        "  batches = int(dataset.shape[0] / batch_size)\n",
        "\n",
        "  #  for each epoch\n",
        "  for i in range(epochs):\n",
        "    print(\"Epoch: \" + str(i))\n",
        "    #for each batch\n",
        "    for j in range(batches):\n",
        "      \n",
        "      #obtain real image data\n",
        "      X_real, y_real = get_real_samples(dataset, batch_size)\n",
        "\n",
        "      #generate artificial data\n",
        "      X_fake, y_fake = generate_fake_samples_using_generator(g_model, noise_dim, batch_size)\n",
        "    \n",
        "      d_loss_real, d_accuracy_real = d_model.train_on_batch(X_real, y_real)\n",
        "      d_loss_fake, d_accuracy_fake = d_model.train_on_batch(X_fake, y_fake)\n",
        "\n",
        "      #X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
        "      #d_loss, d_accuracy = d_model.train_on_batch(X, y)\n",
        "\n",
        "      # generate random vectors of noise (this becomes input the GAN model)\n",
        "      X_gan = generate_random_vectors(noise_dim, batch_size)\n",
        "\n",
        "      # assign all GAN output data vectors a class label of 1\n",
        "      y_gan = np.ones((batch_size, 1))\n",
        "\n",
        "      # train the generator\n",
        "      gan_loss = gan_model.train_on_batch(X_gan , y_gan)\n",
        "\n",
        "      print(\"Batch: \" + str(j) + \" generator loss: \" + str(gan_loss) + \", discriminator real loss: \" + str(d_loss_real)  + \", discriminator fake loss: \" + str(d_loss_fake))\n",
        "      #print(\"Batch: \" + str(j) + \" generator loss: \" + str(gan_loss) + \", discriminator loss: \" + str(d_loss))\n",
        "\n",
        "      drealLoss.append(d_loss_real)\n",
        "      dfakeLoss.append(d_loss_fake)\n",
        "      gLoss.append(gan_loss)\n",
        "      accuracies_real.append(d_accuracy_real)\n",
        "      accuracies_fake.append(d_accuracy_fake)\n",
        "        \n",
        "    if ((i+1) % 10) == 0:\n",
        "\n",
        "      if(os.path.exists(OUTPUT_DIR + \"/\" + str(i)) == False):\n",
        "        os.mkdir(OUTPUT_DIR + \"/\" + str(i))\n",
        "\n",
        "      #g_model.save(\"generatormodelweights\" + str(i) + \".h5\")\n",
        "      #disc_model.save(\"discmodelweights\" + str(i) + \".h5\") \n",
        "\n",
        "      noise_dim = (100, 1)\n",
        "      n_samples = 165\n",
        "      x,y = generate_fake_samples_using_generator(g_model, noise_dim, n_samples)\n",
        "      for m in range(n_samples):\n",
        "        this_image = x[m]\n",
        "        this_image = this_image.astype(np.uint8)   \n",
        "        image_name = str(i) + \"_\" + str(j)+\"_\"+str(m)\n",
        "        im2 = Image.fromarray(this_image)\n",
        "        im2.save(\"{}/{}.jpeg\".format(OUTPUT_DIR + \"/\" + str(i),image_name))\n",
        "\n",
        "    plot_history(drealLoss, dfakeLoss, gLoss)\n",
        "    plot_history_accuracy(accuracies_real, accuracies_fake)\n",
        "\n",
        "    #files.download(\"ganmodelweights\" + str(i) + \".h5\")\n",
        "    #files.download(\"discmodelweights\" + str(i) + \".h5\")    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4mdBnQr0Zjm"
      },
      "source": [
        "noise_dim = (100, 1)\n",
        "shape=(128,128,3)\n",
        "\n",
        "disc_model =  discriminator(shape)\n",
        "lrDisc = 1e-4\n",
        "decayDisc = 1e-5 #2e-5\n",
        "\n",
        "\n",
        "optimizer_disc = SGD(lrDisc)#, decay=decayDisc) #, momentum = 0.9) #, decay=decayDisc)\n",
        "\n",
        "disc_model.compile(loss = 'binary_crossentropy', optimizer=optimizer_disc, metrics=['accuracy'])\n",
        "\n",
        "generator_model = generator()\n",
        "gan_model = gan(generator_model, disc_model)\n",
        "\n",
        "#load image data\n",
        "dataset = load_real_samples(\"/content/gdrive/My Drive/MSc. project/npz files conference/Kaggle_training_images_in_order_preprocessed_class4_normalized3.npz\")\n",
        "\n",
        "#train model\n",
        "epochs=500\n",
        "batch_size= 4 \n",
        "\n",
        "train(generator_model, disc_model, gan_model, dataset, noise_dim, epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY0hry-WkD4N"
      },
      "source": [
        "#print(len(os.listdir('/content/gdrive/My Drive/Kaggle/training preprocessed')))\n",
        "#images = np.load(\"/content/gdrive/My Drive/MSc. project/npz files conference/real + fake/Kaggle_training_images_class4_real_plus_fake_epoch_19.npz\")[\"arr_0\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsq01cMCaLa-"
      },
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "#plt.ion()\n",
        "#plt.figure()\n",
        "#plt.imshow(images[2])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}