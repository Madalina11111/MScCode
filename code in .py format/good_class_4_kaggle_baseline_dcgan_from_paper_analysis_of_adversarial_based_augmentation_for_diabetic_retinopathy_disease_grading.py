# -*- coding: utf-8 -*-
"""GOOD Class 4 Kaggle baseline DCGAN from paper Analysis of Adversarial based Augmentation for Diabetic Retinopathy Disease Grading.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IknChyrHht6FFwuWiqYcAhHmZA4UqBp8
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from matplotlib import pyplot
import os
import keras
from keras.preprocessing.image import load_img, img_to_array
from skimage.io import imread
from PIL import Image #PIL is no longer maintained and it is not compatible with Python 3.X. To use Pillow instead
from numpy import load
from os import listdir
from numpy import savez_compressed
from numpy import asarray
from google.colab import files
from keras import Sequential
from keras.layers import *
from keras import regularizers
from keras.optimizers import Adam, SGD

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

#load images from npz
def load_real_samples(filename):
  data = load(filename)
  X = data['arr_0']
  X = X.astype('float32')
  return X

def discriminator(img_shape):
  depth = 64
  dropout = 0.3
  lrL2 = 1e-7

  D = Sequential()
  D.add(keras.Input(shape=(128,128,3)))

  D.add(Conv2D(depth*1, 5, strides=2, padding='same', kernel_regularizer=regularizers.l2(lrL2)))
  D.add(BatchNormalization())
  D.add(ReLU())
  D.add(Dropout(dropout))
 
  D.add(Conv2D(depth*2, 5, strides=2, padding='same', kernel_regularizer=regularizers.l2(lrL2)))
  D.add(BatchNormalization())
  D.add(ReLU())
  D.add(Dropout(dropout))

  D.add(Conv2D(depth*4, 5, strides=2, padding='same', kernel_regularizer=regularizers.l2(lrL2)))
  D.add(BatchNormalization())
  D.add(ReLU())
  D.add(Dropout(dropout))

  D.add(Conv2D(depth*8, 5, strides=1, padding='same', kernel_regularizer=regularizers.l2(lrL2)))
  D.add(BatchNormalization())
  D.add(ReLU())
  D.add(Dropout(dropout))

  D.add(Flatten())
  D.add(Dense(1, activation="sigmoid"))

  return D

my_disc = discriminator((128, 128, 3))
my_disc.summary()

def generator():

    G = Sequential()
    depth = 64*4
    img_rows = 128
    dim = int(img_rows / 8 )
    dropout = 0.3
    lrL2 = 1e-7

    G.add(keras.Input(shape=(100,1)))
    G.add(Dense(512))
    G.add(BatchNormalization())
    G.add(ReLU())
    G.add(Dropout(dropout))
    G.add(Reshape((dim, dim, 200)))

    G.add(UpSampling2D(size=(2, 2)))

    G.add(Conv2D(int(depth), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))
    G.add(BatchNormalization())
    G.add(ReLU())
    G.add(Dropout(dropout))
    G.add(Conv2D(int(depth/2), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))
    G.add(ReLU())
    G.add(Dropout(dropout))

    G.add(UpSampling2D(size=(2, 2)))

    G.add(Conv2D(int(depth/4), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))
    G.add(BatchNormalization())
    G.add(ReLU())
    G.add(Dropout(dropout))
    G.add(Conv2D(int(depth/8), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))
    G.add(ReLU())
    G.add(Dropout(dropout))

    G.add(UpSampling2D(size=(2, 2)))

    G.add(Conv2D(int(depth/16), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))
    G.add(BatchNormalization())
    G.add(ReLU())
    G.add(Dropout(dropout))
    G.add(Conv2D(int(depth/32), 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))
    G.add(ReLU())
    G.add(Dropout(dropout))

    G.add(Conv2D(3, 5, padding='same', kernel_regularizer=regularizers.l2(lrL2)))
    G.add(ReLU())
    return G

gen_model = generator()
gen_model.summary()

#generate synthetic data using the generator
def generate_fake_samples_using_generator(g_model, noise_dim, n_samples):
  flattened_noise_dim = noise_dim[0]
  x = np.random.randn(n_samples * flattened_noise_dim)
  x_data = x.reshape(n_samples, noise_dim[0], noise_dim[1])
  x = g_model.predict(x_data)
  y = np.zeros((n_samples, 1))

  return x, y

#create GAN architecture
def gan(g_model , d_model):
  d_model.trainable = False
  model = tf.keras.models.Sequential()
  model.add(g_model)
  model.add(d_model)
  lrAdv = 1e-4
  decayAdv = 1e-6
  optimizer_adver = Adam(lrAdv)#, decay = decayAdv)

  model.compile(loss = 'binary_crossentropy', optimizer = optimizer_adver)
  return model

def get_real_samples (dataset, n_samples):
  indicies = np.random.randint(0, dataset.shape[0], n_samples)
  X = dataset[indicies]
  y = np.ones((n_samples, 1))
  return X, y

def generate_random_vectors(noise_dim, batch_size):
  x = np.random.randn(batch_size * noise_dim[0] * noise_dim[1])
  x_data = x.reshape(batch_size, noise_dim[0], noise_dim[1])
  return x_data

def plot_history(d_real_loss, d_fake_loss, g_loss):
  pyplot.subplot(1, 1, 1)
  pyplot.plot(d_real_loss, label='d-real')
  pyplot.plot(d_fake_loss, label='d-fake')
  pyplot.plot(g_loss, label='gen')
  pyplot.legend()
  pyplot.show()
  pyplot.close()

def plot_history_accuracy(d_real_acc, d_fake_acc):
  pyplot.subplot(1, 1, 1)
  pyplot.plot(d_real_acc, label='acc-real')
  pyplot.plot(d_fake_acc, label='acc-fake')
  pyplot.legend()
  pyplot.show()
  pyplot.close()

#train the gan
import cv2
OUTPUT_DIR = "/content/gdrive/My Drive/img_retina_Kaggle_class_4_state_of_art_GAN"

#train the generator and discriminator
def train(g_model, d_model, gan_model, dataset, noise_dim, epochs, batch_size):
  drealLoss = []
  dfakeLoss = []
  gLoss = []
  accuracies_real = []
  accuracies_fake = []
  inx = []
  counter = 0
  batches = int(dataset.shape[0] / batch_size)

  #  for each epoch
  for i in range(epochs):
    print("Epoch: " + str(i))
    for j in range(batches):
      X_real, y_real = get_real_samples(dataset, batch_size)
      X_fake, y_fake = generate_fake_samples_using_generator(g_model, noise_dim, batch_size)
      d_loss_real, d_accuracy_real = d_model.train_on_batch(X_real, y_real)
      d_loss_fake, d_accuracy_fake = d_model.train_on_batch(X_fake, y_fake)
      X_gan = generate_random_vectors(noise_dim, batch_size)
      y_gan = np.ones((batch_size, 1))
      gan_loss = gan_model.train_on_batch(X_gan , y_gan)
      print("Batch: " + str(j) + " generator loss: " + str(gan_loss) + ", discriminator real loss: " + str(d_loss_real)  + ", discriminator fake loss: " + str(d_loss_fake))
      drealLoss.append(d_loss_real)
      dfakeLoss.append(d_loss_fake)
      gLoss.append(gan_loss)
      accuracies_real.append(d_accuracy_real)
      accuracies_fake.append(d_accuracy_fake)
        
    if ((i+1) % 10) == 0:

      if(os.path.exists(OUTPUT_DIR + "/" + str(i)) == False):
        os.mkdir(OUTPUT_DIR + "/" + str(i))
      noise_dim = (100, 1)
      n_samples = 165
      x,y = generate_fake_samples_using_generator(g_model, noise_dim, n_samples)
      for m in range(n_samples):
        this_image = x[m]
        this_image = this_image.astype(np.uint8)   
        image_name = str(i) + "_" + str(j)+"_"+str(m)
        im2 = Image.fromarray(this_image)
        im2.save("{}/{}.jpeg".format(OUTPUT_DIR + "/" + str(i),image_name))

    plot_history(drealLoss, dfakeLoss, gLoss)
    plot_history_accuracy(accuracies_real, accuracies_fake)

noise_dim = (100, 1)
shape=(128,128,3)

disc_model =  discriminator(shape)
lrDisc = 1e-4
decayDisc = 1e-5 #2e-5


optimizer_disc = SGD(lrDisc)#, decay=decayDisc) #, momentum = 0.9) #, decay=decayDisc)

disc_model.compile(loss = 'binary_crossentropy', optimizer=optimizer_disc, metrics=['accuracy'])

generator_model = generator()
gan_model = gan(generator_model, disc_model)

dataset = load_real_samples("/content/gdrive/My Drive/MSc. project/npz files conference/Kaggle_training_images_in_order_preprocessed_class4_normalized3.npz")

epochs=500
batch_size= 4 

train(generator_model, disc_model, gan_model, dataset, noise_dim, epochs, batch_size)